# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUBAzbZw7Npp6m7h86o1llyRhZ9l5LH3
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install xgboost Flask

import xgboost as xgb
from flask import Flask

print("Libraries installed and imported successfully!")

# Data Manipulation and Analysis
import pandas as pd
import numpy as np

# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier

# Other Libraries
import warnings
warnings.filterwarnings('ignore')

# Load dataset
data = pd.read_csv('/Telecom Customers Churn.csv')

# Display the first few rows
print(data.head())

# Check the shape of the dataset
print("Shape of dataset:", data.shape)

# Get basic info
print(data.info())

# Check for missing values
print("Missing values per column:")
print(data.isnull().sum())

# Statistical summary
print(data.describe())

data.to_csv('/content/Telecom Customers Churn.csv', index=False)
print("Processed dataset saved.")

# Import pandas
import pandas as pd

# Load dataset (ensure correct path)
data = pd.read_csv('/content/Telecom Customers Churn.csv')

# Preview data
print("Dataset Preview:")
print(data.head())

# Check dataset dimensions
print("Dataset Shape:", data.shape)

# Check for missing values
print("Missing Values:")
print(data.isnull().sum())

# Handle missing values (drop or fill)
data = data.dropna()  # Dropping rows with missing values
print("Shape after removing missing values:", data.shape)

from sklearn.preprocessing import LabelEncoder

# Identify categorical columns
categorical_columns = data.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_columns)

# Apply Label Encoding
encoder = LabelEncoder()
for col in categorical_columns:
    data[col] = encoder.fit_transform(data[col])

print("Encoded Dataset Preview:")
print(data.head())

from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# Separate features (X) and target (y)
X = data.drop(columns=['Churn'])  # Exclude the target column from features
y = data['Churn']  # Target variable

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

print("After SMOTE - X shape:", X_resampled.shape, "y shape:", y_resampled.shape)

from sklearn.model_selection import train_test_split

# Re-split the data after SMOTE
X_train_resampled, X_test_resampled, y_train_resampled, y_test_resampled = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

print("Resampled Training Set Shape:", X_train_resampled.shape, y_train_resampled.shape)
print("Resampled Testing Set Shape:", X_test_resampled.shape, y_test_resampled.shape)

from collections import Counter

print("Original class distribution:", Counter(y))
print("Resampled class distribution:", Counter(y_resampled))

# Logistic Regression
lr_model.fit(X_train_resampled, y_train_resampled)

# Random Forest
rf_model.fit(X_train_resampled, y_train_resampled)

# XGBoost
xgb_model.fit(X_train_resampled, y_train_resampled)

from imblearn.over_sampling import SMOTE

# Feature engineering: Adding interaction terms
data['Tenure_MonthlyCharges'] = data['tenure'] * data['MonthlyCharges']

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
print("After SMOTE - X shape:", X_resampled.shape, "y shape:", y_resampled.shape)

# Separate features (X) and target (y)
X = data.drop("Churn", axis=1)
y = data["Churn"]

print("Features Shape:", X.shape)
print("Target Shape:", y.shape)

from sklearn.preprocessing import StandardScaler

# Apply StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Feature Scaling Completed. Scaled Feature Shape:", X_scaled.shape)

# Check the shapes of X and y
print("Features (X) Shape:", X.shape)
print("Target (y) Shape:", y.shape)

from sklearn.model_selection import train_test_split

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("Training Set Shape:", X_train.shape, y_train.shape)
print("Testing Set Shape:", X_test.shape, y_test.shape)

import seaborn as sns
import matplotlib.pyplot as plt

# Plotting the correlation heatmap
plt.figure(figsize=(12,8))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

# Import necessary libraries
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# 1. Apply SMOTE to handle class imbalance
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# 2. Train a model (using Logistic Regression or Random Forest, can be replaced with your model)
model = RandomForestClassifier(random_state=42)
model.fit(X_train_resampled, y_train_resampled)

# 3. Predict probabilities (for ROC Curve)
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability for the positive class (churn)

# 4. Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# 5. Plot ROC curve
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Print ROC AUC score
print(f"ROC AUC Score: {roc_auc:.2f}")

# Churn distribution visualization
sns.countplot(data=data, x='Churn')
plt.title("Churn Distribution")
plt.xlabel("Churn")
plt.ylabel("Count")
plt.show()

# Churn vs Tenure
sns.boxplot(x='Churn', y='tenure', data=data)
plt.title("Churn vs Tenure")
plt.show()

# Churn vs Monthly Charges
sns.boxplot(x='Churn', y='MonthlyCharges', data=data)
plt.title("Churn vs Monthly Charges")
plt.show()

# Check the range of the MonthlyCharges column
print(data['MonthlyCharges'].describe())

# Categorize Tenure
data['tenure_group'] = pd.cut(
    data['tenure'],
    bins=[0, 1, 3, 5, 10, 20],
    labels=['0-1', '1-3', '3-5', '5-10', '10+']
)

# Verify new column
print(data[['tenure', 'tenure_group']].head())

# Churn vs Tenure Groups
sns.countplot(x='tenure_group', hue='Churn', data=data)
plt.title("Churn by Tenure Groups")
plt.xlabel("Tenure Group")
plt.ylabel("Count")
plt.show()

# Categorize Monthly Charges
data['monthly_charges_group'] = pd.cut(
    data['MonthlyCharges'],
    bins=[0, 20, 50, 75, 100, 200],
    labels=['0-20', '20-50', '50-75', '75-100', '100+']
)

# Verify new column
print(data[['MonthlyCharges', 'monthly_charges_group']].head())

# Churn vs Monthly Charges Groups
sns.countplot(x='monthly_charges_group', hue='Churn', data=data)
plt.title("Churn by Monthly Charges Group")
plt.xlabel("Monthly Charges Group")
plt.ylabel("Count")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest to check feature importance
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_scaled, y)

# Get feature importance
importances = rf_model.feature_importances_
features = X.columns

# Create a DataFrame to view the importance of features
feature_importance = pd.DataFrame({'feature': features, 'importance': importances})
feature_importance = feature_importance.sort_values(by='importance', ascending=False)

# Plot the feature importance
plt.figure(figsize=(10,6))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title("Feature Importance")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# XGBoost
xgb_model = xgb.XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score

# Logistic Regression Evaluation
lr_cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')
print("Logistic Regression CV Accuracy:", lr_cv_scores.mean())

# Random Forest Evaluation
rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')
print("Random Forest CV Accuracy:", rf_cv_scores.mean())

# XGBoost Evaluation
xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')
print("XGBoost CV Accuracy:", xgb_cv_scores.mean())

from sklearn.metrics import classification_report, roc_auc_score

# Logistic Regression Evaluation
y_pred_lr_resampled = lr_model.predict(X_test_resampled)
print("Logistic Regression Classification Report (SMOTE):\n", classification_report(y_test_resampled, y_pred_lr_resampled))
print("Logistic Regression ROC-AUC Score (SMOTE):", roc_auc_score(y_test_resampled, y_pred_lr_resampled))

# Random Forest Evaluation
y_pred_rf_resampled = rf_model.predict(X_test_resampled)
print("Random Forest Classification Report (SMOTE):\n", classification_report(y_test_resampled, y_pred_rf_resampled))
print("Random Forest ROC-AUC Score (SMOTE):", roc_auc_score(y_test_resampled, y_pred_rf_resampled))

# XGBoost Evaluation
y_pred_xgb_resampled = xgb_model.predict(X_test_resampled)
print("XGBoost Classification Report (SMOTE):\n", classification_report(y_test_resampled, y_pred_xgb_resampled))
print("XGBoost ROC-AUC Score (SMOTE):", roc_auc_score(y_test_resampled, y_pred_xgb_resampled))

from sklearn.model_selection import cross_val_score

# Logistic Regression Evaluation (using resampled data)
lr_cv_scores = cross_val_score(lr_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("Logistic Regression CV Accuracy (SMOTE):", lr_cv_scores.mean())

# Random Forest Evaluation (using resampled data)
rf_cv_scores = cross_val_score(rf_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("Random Forest CV Accuracy (SMOTE):", rf_cv_scores.mean())

# XGBoost Evaluation (using resampled data)
xgb_cv_scores = cross_val_score(xgb_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("XGBoost CV Accuracy (SMOTE):", xgb_cv_scores.mean())

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Logistic Regression Confusion Matrix
cm_lr = confusion_matrix(y_test, y_pred_lr)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Churned', 'Churned'], yticklabels=['Not Churned', 'Churned'])
plt.title("Logistic Regression Confusion Matrix")
plt.show()

# Random Forest Confusion Matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Churned', 'Churned'], yticklabels=['Not Churned', 'Churned'])
plt.title("Random Forest Confusion Matrix")
plt.show()

# XGBoost Confusion Matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Churned', 'Churned'], yticklabels=['Not Churned', 'Churned'])
plt.title("XGBoost Confusion Matrix")
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the parameter grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],        # Number of trees in the forest
    'max_depth': [10, 20, 30, None],        # Maximum depth of each tree
    'min_samples_split': [2, 5, 10],        # Minimum samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],          # Minimum samples required at a leaf node
    'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when splitting
}

# Apply RandomizedSearchCV
search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_grid,
                            n_iter=10, cv=5, random_state=42, verbose=2)
search.fit(X_train, y_train)

# Get the best model from the search
best_model = search.best_estimator_

# Check the best hyperparameters
print("Best Hyperparameters:", search.best_params_)

print("Best Hyperparameters:", search.best_params_)

# Predictions with the best model (on resampled data)
y_pred_best_smote = best_model.predict(X_resampled)

# Evaluate the best model
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

print("Best Model (SMOTE) Classification Report:\n", classification_report(y_resampled, y_pred_best_smote))
print("Best Model (SMOTE) ROC-AUC Score:", roc_auc_score(y_resampled, y_pred_best_smote))

# Confusion Matrix for Best Model (SMOTE)
cm_best_smote = confusion_matrix(y_resampled, y_pred_best_smote)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_best_smote, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Churned', 'Churned'], yticklabels=['Not Churned', 'Churned'])
plt.title("Best Model (SMOTE) Confusion Matrix")
plt.show()

# Generate predicted probabilities instead of class labels
y_pred_prob_rf = best_model.predict_proba(X_test)[:, 1]  # Getting probabilities for the positive class

# Compute ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob_rf)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Import necessary libraries
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt
import joblib

# Apply SMOTE to balance the training data
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
print(f"Resampled Training Set Shape: X = {X_resampled.shape}, y = {y_resampled.shape}")

# Train the best model (Random Forest in this case)
best_model = RandomForestClassifier(random_state=42)
best_model.fit(X_resampled, y_resampled)

# Predict on the test set (use the original test set, not resampled)
y_pred_best = best_model.predict(X_test)

# Evaluate the best model
print("Best Model Classification Report (with SMOTE):\n", classification_report(y_test, y_pred_best))
print("Best Model ROC-AUC Score (with SMOTE):", roc_auc_score(y_test, y_pred_best))

# Print final accuracy score
print(f"Final Accuracy of the best model (with SMOTE): {best_model.score(X_test, y_test):.2f}")